\documentclass{cognito}
\usepackage{lmodern}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{dsfont}

\begin{document}

% Some options
\lstset{language=Python}
\lstset{mathescape=true}

% For links use
% \hyperref[note:Heuristic Minimax]{\emph{Heuristic Minimax}}

\begin{note}{Artificial Intelligence}
	\tags{Daniel Balle 2018}
\end{note}

%%%% PART II : Problem-Solving

%%% 3 Searching

\begin{note}{Classical Search}
	Path search problems are well-defined problems in which the solution is a \emph{path}
	or action sequence from an initial state to a goal state. All possible
	sequences form a search tree.
	\vspace{5pt}
	
	All path search algorithms share the same structure, but vary primarily
	according to how they choose which state to expand next - their \emph{search strategy}.
	\tags{Search, Path Search}
\end{note}

%% 3.4 Uninformed search

\begin{note}{Graph Search}
	Graph search is a general search algorithm
	similar to  tree search that remembers his past exploration.
	\begin{largecode}
explored = Set(); frontier = {start state}
while (frontier):
 	x = frontier.pop_using(strategy)
	if (x is goal state): return solution
	explored.add(x)
	for (n in x.successors() if n not in explored):
		frontier.add(n)  # or update
return False
	\end{largecode}
	\vspace{-5pt}
	\tags{Search, Path Search, Uninformed Search}
\end{note}

\begin{note}{Graph Search Implementation}
	The \hyperref[note:Graph Search]{\emph{graph search}} algorithm above can be extended to allow any 
	search strategy by using a priority queue as the frontier. Each element in the
	priority should be a sequence of
	\begin{itemize}
		\item the node $n$ itself
		\item the path-cost $g(n)$
		\item the path that explored this node
	\end{itemize}
	\begin{remark} Our queue retrieves elements with lowest weights first.
	\end{remark}
	\vspace{-5pt}
	\tags{Search, Path Search, Uninformed Search}
\end{note}

\begin{note}{Breadth-First Search}
	\begin{mdframed}[linecolor=black!25!white]
		search strategy: time
	\end{mdframed}
	BFS is a \hyperref[note:Graph Search]{\emph{graph search}} algorithm using a FIFO queue for the frontier, thus the
	shallowest node is chosen for expansion at every step.
	The goal state test can also be applied during discovery rather than expansion.
	
	\begin{remark} BFS is only optimal when the path cost is a nondecreasing function
		of the depth of the node. It is complete under finite branching factor $b$.
	\end{remark}
	\vspace{-5pt}
	\tags{Search, Path Search, Uninformed Search}
\end{note}

\begin{note}{Uniform-Cost Search}
	\begin{mdframed}[linecolor=black!25!white]
		search strategy: path cost $g(n)$
	\end{mdframed}
	UCS is a generic \hyperref[note:Graph Search]{\emph{graph search}} algorithm which expands the node $n$ with lowest path cost $g(n)$ first.
	\begin{remark} It is optimal for any path cost. \end{remark}
	\begin{remark} See \emph{Dijkstra's} algorithm from \textbf{Algorithms} \noteref notes. \end{remark}
	\vspace{-5pt}
	\tags{Search, Path Search, Uninformed Search}
\end{note}

\begin{note}{Depth-First Search}
	\begin{mdframed}[linecolor=black!25!white]
		search strategy: -time
	\end{mdframed}
	DFS is a \hyperref[note:Graph Search]{\emph{graph search}} algorithm using a stack,
	it thus always expands the deepest node in the frontier. 
	\begin{remark} DFS is not optimal. Its tree search isn't even complete.
	\end{remark}
	\vspace{-5pt}
	\tags{Search, Path Search, Uninformed Search}
\end{note}

\begin{note}{Iterative Deepening DFS}
	Iterative Deepening search performs a \emph{depth-limited} \hyperref[note:Depth-First Search]{\it depth-first search}
	by gradually increasing the limit.
	\begin{remark}
		Optimality and completeness as BFS. 
	\end{remark}
	\vspace{-5pt}
	\tags{Search, Path Search, Uninformed Search}
\end{note}

%% 3.5 Informed Search
\begin{note}{Best-First Search}
	\begin{mdframed}[linecolor=black!25!white]
		search strategy: $f(n)$
	\end{mdframed}
	Best-First Search is a general \emph{informed} \hyperref[note:Graph Search]{\emph{graph search}} algorithm using
	an evaluation function $f(n)$ as its search strategy, usually involving a
	\emph{heuristic} function:
	$$
		h(n) = \text{estimated cost of optimal path from $n$ to goal}
	$$
	\vspace{-10pt}
	\tags{Search, Path Search, Informed Search}
\end{note}


\begin{note}{Greedy Best-First Search}
	\begin{mdframed}[linecolor=black!25!white]
		search strategy: $f(n) = h(n)$
	\end{mdframed}
	Greedy Search is an \hyperref[note:Best-First Search]{\it informed search} algorithm that expands nodes that are estimated
	to be the closest to the goal using some heuristic function $h(n)$.
	\begin{remark} It is neither optimal nor complete. \end{remark}
	\vspace{-5pt}
	\tags{Search, Path Search, Informed Search}
\end{note}

\begin{note}{A* Search}
	\begin{mdframed}[linecolor=black!25!white]
		search strategy: $f(n) = g(n) + h(n)$
	\end{mdframed}
	A* Search is an \hyperref[note:Best-First Search]{\it informed search} algorithm using
	an estimated cost of the cheapest solution through $n$ as its
	evaluation function.
	\begin{remark} It is both complete and optimal when $h$ is \hyperref[note:Heuristic Admissibility]{\it admissible} for tree search,
		or \hyperref[note:Heuristic Consistency]{\it consistent} for graph search.
		A* is also optimally efficient in terms of nodes expanded.
	\end{remark}
	\vspace{-5pt}
	\tags{Search, Path Search, Informed Search}
\end{note}

\begin{note}{Iterative Deepening A*}
	IDA* is an application of \hyperref[note:Iterative Deepening DFS]{\it iterative deepening} to \hyperref[note:A* Search]{\it A* search}
	using a cutoff value
	for the evaluation function $f(n)$. At each iterations the new cutoff
	value is the lowest $f(n)$ that previously exceeded the cutoff.
	
	\tags{Search, Path Search, Informed Search}
\end{note}

% Recursive Best-First Search
% ? A* optimality proof ?

\begin{note}{Heuristic Admissibility} 
	A heuristic $h$ is admissible if it never overestimates the
	true optimal path from $n$ to the goal $h^*(n)$.
	$$ h(n) \leq h^*(n) \quad \forall n \in V$$
	\vspace{-10pt}
	\tags{Search, Path Search, Informed Search, Heuristics}
\end{note}

\begin{note}{Heuristic Consistency} 
	A heuristic $h$ is consistent if for any successors $n'$ of $n$ reached through action $a$, it holds that: 
	$$ h(n) \leq c(n, a, n') + h(n') \quad \forall (n, n') \in E$$
	\begin{remark} Consistency implies \hyperref[note:Heuristic Admissibility]{\it admissibility}. \end{remark}
	\vspace{-5pt}
	\tags{Search, Path Search, Informed Search, Heuristics}
\end{note}

\begin{answer}{Path Search Todo}
	\begin{itemize}
		\item Runtime and Memory properties
		\item Optimality proof of A*
		\item Recursive Best-First Search, MA*, SMA*
	\end{itemize}
	\tags{Search, Path Search}
\end{answer}


%%% 4 Optimization and Local Search
\begin{note}{Optimization and Local Search}
	Local search algorithms operate using a single current node and solve
	optimization problems whose solution is a \emph{state} $s^* \in S$ according
	to some objective function $f(s)$.

	\tags{Search, Local Search \& Optimization}
\end{note}

\begin{note}{Hill-Climbing Search}
	\begin{mdframed}[linecolor=black!25!white]
		$$
			x^{(t+1)} \leftarrow x^{(t)} + \eta \nabla f(x^{(t)})
			\quad\quad \eta > 0
		$$
	\end{mdframed}
	Hill-climbing search is a greedy \hyperref[note:Optimization and Local Search]{\it local search} algorithm which continually moves in the
	direction of increasing value. Variations include:
	\begin{description}
		\item[Stochastic:] choose randomly among all uphill moves.
		\item[First-choice:] randomly generate successors until a better state than the current one is found.
		\item[Random-restart:] Multiple tries with random initial states.
	\end{description}
	\vspace{-5pt}
	\tags{Search, Local Search \& Optimization}
\end{note}

\begin{note}{Simulated Annealing}
	This \hyperref[note:Optimization and Local Search]{\it local search} algorithm picks a random move $n$:
	\begin{mdframed}[linecolor=black!25!white]
	\begin{itemize}
		\item if the move improves the state, it accepts
		\item otherwise it accepts with probability $p = e^{\Delta E / T}$
	\end{itemize}
	\end{mdframed}
	with $\Delta E$ the amount by which the state is worsened,
	and $T$ the temperature at time $t$.
	\tags{Search, Local Search \& Optimization}
\end{note}

\begin{answer}{Local Search Todo}
	\begin{itemize}
		\item Local Beam Search
		\item Genetic Algorithms
		\item Continuous local search
		\item Constraint Search Problem, Integer Programming
		\item AND-OR Search
		\item Online vs Offline Search
		\item All the rest ...
	\end{itemize}
	\tags{Search, Local Search \& Optimization}
\end{answer}

\begin{answer}{Todo}
	\begin{enumerate}
		\item Reorganize Optimization
		\item More ML vocabulary
		\item SGD etc
	\end{enumerate}
\end{answer}

%%% Adverserial Search

\begin{note}{Adversarial Search}
	Games are adversarial search problems in a competitive multi-agent environment
	whose solution is a \emph{strategy} $f : S \to A$.
	%The solution to a game is a \emph{strategy} $f : S \to A$.
	\vspace{5pt}
	
	Similar to search, the sequences of actions from the initial game state generate a \emph{game tree}.
	A utility functions defines the final value of a terminal state for each player.
	\begin{remark} Games can also be seen as \hyperref[note:Stochastic Decision Problems]{\it decision problems}.
	\end{remark}
	\vspace{-5pt}
	\tags{Search, Adversarial Search}
\end{note}

% Reflex agent

\begin{note}{Minimax Decision}
	The minimax decision in a given game state $s$ is choosing the optimal action $a$,
	assuming opponents play optimally. We maximize the worst-case outcome, based
	on the \emph{minimax value}:
	$$
		mv(s) = \begin{cases*}
			\text{utility}(s) & if $s$ is terminal\\
			\max_a \, mv(s + a) & if player is \textit{Max}\\
			\min_a \, mv(s + a) & if player is \textit{Min}
		\end{cases*}
	$$
	\vspace{-5pt}
	\tags{Search, Adversarial Search, Minimax}
\end{note}

\begin{note}{Minimax Algorithm}
	Given a current state $s$, the minimax algorithm performs the \hyperref[note:Minimax Decision]{\emph{minimax decision}}.
	This results in a complete \hyperref[note:Depth-First Search]{\emph{depth-first}} search exploration of the game tree.
%	\begin{code}
%for action in game.getActions(state):
%	value = minimaxValue(game.getSuccessor(state, action))
%	if (value > best.value): best = Decision(action, value)
%return action
%	\end{code}
		\begin{code}
decisions = { 
	action : minimaxValue(game.getSuccessor(state, action))
	for action in game.getActions(state)
}
return max(decisions, key=decisions.get)
	\end{code}
	\begin{remark}
		In practice we use a \emph{ply} limited search: \hyperref[note:Heuristic Minimax]{\it heuristic minimax}.
	\end{remark}
	\vspace{-5pt}
	\tags{Search, Adversarial Search, Minimax}
\end{note}

\begin{note}{Alpha Beta Pruning}
	$\alpha$-$\beta$ pruning is an optimization technique for \hyperref[note:Minimax Algorithm]{\it minimax search}
	which prunes away branches in the minimax tree that won't change the final decision, using additional variables:
	\begin{itemize}
		\item $\alpha =$ value of best decision by $Max$ on path to root
		\item $\beta =$ value of best decision by $Min$ on path to root
	\end{itemize}
	\tags{Search, Adversarial Search, Minimax}
\end{note}

\begin{note}{Alpha Beta Implementation}
	The \hyperref[note:Alpha Beta Pruning]{\it $\alpha$-$\beta$ pruning} algorithm \incode{minimaxValue(s, $\alpha$, $\beta$)} for player \textit{Min} can be implemented as follows.
	\begin{code}
for next in game.getSuccessors(state):
	value = min(value, minimaxValue(next, $\alpha$, $\beta$))
	if (value < $\alpha$) : return value
	$\beta$ = min($\beta$, value)
return value
	\end{code}
	Because $Max$ can choose an action yielding $\alpha$, this subtree won't be considered for his
	final decision if we yield a value $< \alpha$.
	
	\tags{Search, Adversarial Search, Minimax}
\end{note}

\begin{note}{Heuristic Minimax}
	H-minimax is a depth or ply limited \hyperref[note:Minimax Algorithm]{\it minimax search} using a heuristic evaluation function
	for non-terminal states.
	$$
		hm(s, d) = \begin{cases*}
			\text{eval}(s) & if cutoff$(s, d)$\\
			\max_a  \, hm(s + a, d + 1) & if player is \textit{Max}\\
			\min_a \, hm(s + a, d + 1) & if player is \textit{Min}
		\end{cases*}
	$$
	\begin{remark} This is used in practice for imperfect real-time decisions.
		Another approach is using \hyperref[note:Iterative Deepening DFS]{\it iterative deepening} until time runs out.
	\end{remark}
	\vspace{-5pt}
	\tags{Search, Adversarial Search, Minimax}
\end{note}

% Quiescence search
% Horizon Effect and singular extension
% The rest of 5.4 and onwards

\begin{note}{Expecti-Minimax Value}
	Expecti-minimax is an extension of \hyperref[note:Minimax Decision]{\it minimax} to stochastic games in which
	we use the expected value for states played by a \emph{chance} player.
	$$
		em(s) = \begin{cases*}
			\sum_a \, p(a) em(s + a) & if player is \textit{Chance}\\
			mv(s) & otherwise
		\end{cases*}
	$$
	\vspace{-5pt}
	\tags{Search, Adversarial Search, Minimax}
\end{note}

\begin{note}{Bayesian Networks}
	A Bayesian network is a locally structured representation of a \emph{joint probability} distribution
	as a directed acyclic graph where:\begin{itemize}
		\item each node $X$ is a random variable
		\item annotated with \emph{conditional} probability distribution given its incoming nodes or parents $\mathbf{P}(X|\text{Parents}(X))$
	\end{itemize}
	Each node $X$ is conditionally independent of its \emph{non-descendants} given his parents, and all other nodes
	given his \hyperref[note:Markov Blanket]{\emph{Markov Blanket}}.
	
	\tags{Uncertainty, Probabilistic Reasoning}
\end{note}

\begin{note}{Markov Blanket}
	In a \hyperref[note:Bayesian Networks]{\emph{Bayesian network}}, the Markov blanket $\partial X$ of a node $X$ is the set including his \emph{parents}, \emph{children}, and \emph{children's parents}.
	$$
		X \perp \!\!\!\! \perp B \mid \partial X \quad \text{or} \quad P(X \mid \partial X, B) = P(X \mid \partial X)
	$$
	\vspace{-10pt}
	\tags{Uncertainty, Probabilistic Reasoning, Bayesian Networks}
\end{note}

\begin{note}{Chain Rule}
	The chain rule can be used to compute any \emph{joint} distribution given only conditional probabilities,
	e.g. by a \hyperref[note:Bayesian Networks]{\it Bayesian network}:
	$$
		P( X_1,..., X_n) = \prod_{i=1}^n P(X_i \mid X_{i-1}, ..., X_1)
	$$
	\vspace{-10pt}
	\tags{Uncertainty, Probabilistic Reasoning, Bayesian Networks}
\end{note}

\begin{note}{D-Separation}
	In a \hyperref[note:Bayesian Networks]{\it Bayesian network},
	a path $p$ \emph{d-separates} $X$ and $Y$ by set $Z$ containing variable $M$ if \emph{any} of these conditions hold:
	\begin{itemize}
		\item $p$ contains a directed chain $X \to ... \to M \to ... \to Y$
		\item $p$ contains a fork $X \to ... \to M \leftarrow ... \leftarrow Y$
		\item $p$ contains an inverted fork $X \leftarrow ... \leftarrow N \to ... \to Y$, where neither $N$ nor any of its descendants $\in Z$
	\end{itemize}
	If \emph{all} paths $p$ from $X$ to $Y$ are d-separated, then $X \perp \!\!\!\! \perp Y \mid Z$.
	\tags{Uncertainty, Probabilistic Reasoning, Bayesian Networks}
\end{note}

% Continuous Bayesian Networks?

\begin{note}{Probabilistic Inference}
	Probabilistic inference computes the \emph{posterior} distribution of a set of \emph{query} variables
	given an assignment of \emph{evidence} variables.
	$$
		\mathbf{P}(X\mid \mathbf{e})
	$$
	\begin{remark} We usually denote by $\mathbf{Y}$ the \emph{hidden} variables. \end{remark}
	\vspace{-5pt}
	\tags{Uncertainty, Probabilistic Reasoning, Inference}
\end{note}

\begin{note}{Inference by Enumeration}
	IE is an exact \hyperref[note:Probabilistic Inference]{\it inference} technique which
	selects the terms consistent with the evidence $\mathbf{e}$ from the \emph{joint} distribution to sum out the hidden variables before normalizing.
	$$
		\textstyle \mathbf{P}(X\mid \mathbf{e}) = \alpha \mathbf{P}(X, \mathbf{e}) = \alpha \sum_\mathbf{y} \mathbf{P}(X, \mathbf{e}, \mathbf{y}) 
	$$
	\begin{remark} Given a \hyperref[note:Bayesian Networks]{\emph{Bayesian network}}, $\mathbf{P}(X, \mathbf{e}, \mathbf{y})$
	can be computed using the \hyperref[note:Chain Rule]{\it chain rule} on the conditional probabilities.
	\end{remark}
	\vspace{-5pt}
	\tags{Uncertainty, Probabilistic Reasoning, Inference}
\end{note}

\begin{note}{Variable Elimination}
	VE is a dynamic programming algorithm for exact \hyperref[note:Probabilistic Inference]{\it inference}
	summing out the hidden variables $\mathbf{y}$ right-to-left
	and storing intermediate \emph{factors} $\mathbf{f}_i$. 
	$$
	 \textstyle \mathbf{P}(X\mid \mathbf{e}) = \alpha \sum_{y_1} \mathbf{f}_2(y_1, X, \mathbf{e}_2) \times \sum_{y_2} \mathbf{f}_1(y_2, X, \mathbf{e}_1)
	$$
	\begin{remark} $\times$ is point-wise product operator as $\mathbf{f}_i$ are matrices.\end{remark}
	\vspace{-5pt}
	\tags{Uncertainty, Probabilistic Reasoning, Inference}
\end{note}

\begin{note}{Direct Sampling}
	Direct or prior sampling for \hyperref[note:Bayesian Networks]{\it Bayesian networks} is a randomized sampling algorithm which samples
	each variable in \emph{topological} order, conditioned on the sampled values of its parents.
	\begin{code}
for $X_i$ in Topologic[$X_1$, ..., $X_n$] do
	$\mathbf{x}[i]$ $\leftarrow$ random sample from $\mathbf{P}(X_i | $parents$(X_i) )$
	\end{code}
	The \emph{consistent} estimate of the \emph{joint} probability is then given by
	$$
		\textstyle S_{PS}(\mathbf{x}) = N_{PS}(\mathbf{x}) \cdot N^{-1} \xrightarrow{N \to \infty } P(\mathbf{x})
	$$
	%\begin{remark} the topological order is given by the Bayesian network.\end{remark}
	\begin{remark} This is an instance of a \emph{Monte Carlo} algorithm.\end{remark}
	\vspace{-5pt}
	\tags{Uncertainty, Probabilistic Reasoning, Sampling}
\end{note}

\begin{note}{Rejection Sampling}
	Rejection Sampling performs \hyperref[note:Direct Sampling]{\emph{prior sampling}}
	and then rejects all samples that do not match the evidence $\mathbf{e}$.
	$$
		\textstyle \mathbf{\hat{P}}(X | \mathbf{e}) = \mathbf{N}_{RS}(X, \mathbf{e}) \cdot N_{PS}(\mathbf{e})^{-1}
	$$
	\begin{remark} Rejection sampling produces a consistent estimate.\end{remark}
	\vspace{-5pt}
	\tags{Uncertainty, Probabilistic Reasoning, Sampling}
\end{note}

\begin{note}{Likelihood Weighting}
	Likelihood weighting is an extension to  \hyperref[note:Direct Sampling]{\it prior sampling} which
	samples only the non-evidence variables, and weighs each sample according to
	its \emph{likelihood} given the evidence.
	\begin{code}
for $X_i$ in Topologic[$X_1$, ..., $X_n$] do
	if $X_i$ is evidence in $\mathbf{e}$ with value $x_i$ do
		$\mathbf{x}[i]$ $\leftarrow$ $x_i$; $w \leftarrow w \cdot P(X_i = x_i \mid $parents$(X_i))$  # initially 1
	else $\mathbf{x}[i]$ $\leftarrow$ random sample from $\mathbf{P}(X_i | $parents$(X_i) )$
	\end{code}
	$$
		\textstyle \hat{P}(x | \mathbf{e}) = \alpha \sum_\mathbf{y} N(x, \mathbf{y}, \mathbf{e}) \cdot w(x, \mathbf{y}, \mathbf{e})
	$$
	\begin{remark} This is an instance of importance sampling.\end{remark}
	\vspace{-5pt}
	\tags{Uncertainty, Probabilistic Reasoning, Sampling}
\end{note}

\begin{note}{Gibbs Sampling}
	From an arbitrary sample $s_1$ with correct evidence $\mathbf{e}$, generate the next sample $s_2$ by sampling
	one of the non-evidence variables $X_i$
	conditioned on the values of its \hyperref[note:Markov Blanket]{\emph{Markov Blanket}} $\partial X_i$ in $s_1$.
	$$ x_i \sim \mathbf{P}(X_i \mid \partial X_i \in s_1)$$
	\begin{remark} This is a \emph{Markov Chain Monte Carlo} algorithm.
	\end{remark}
	\vspace{-5pt}
	\tags{Uncertainty, Probabilistic Reasoning, Sampling}
\end{note}

% D-Separation

% RELATIONAL AND FIRST-ORDER PROBABILITY MODELS

%  value-of-perfect-information computations ?


\begin{note}{Stochastic Decision Problems}
	A solution to a sequential decision problem in a stochastic environment
	%where actions are non-deterministic
	is a \emph{policy} mapping
	every state $s$ to an action $a$:
	$$
		\pi : S \to A
	$$
	\begin{remark} Partially observable environments require a mapping from belief states $b$ instead $\pi : \Delta(S) \to A$.
	\end{remark}
	\vspace{-5pt}
	\tags{Uncertainty, Policy Search}
\end{note}

\begin{note}{Policy and Utility}
	We denote by $V^*(s)$ or $U^*(s)$ the expected sum of future rewards, or \emph{utility}, when acting optimally starting from state $s$.
	The utility of acting under policy $\pi$ is denoted as $V^\pi(s)$.
	An \emph{optimal policy} $\pi^*$ is one that yields the highest expected utility:
	$$
		\pi^*_s = \text{arg}\max_\pi V^\pi(s)
	$$
	\vspace{-10pt}
	\tags{Uncertainty, Policy Search}
\end{note}

\begin{note}{Planning vs. Learning}
	Algorithms to solve \hyperref[note:Stochastic Decision Problems]{\it decision problems} can be categorized based on the environment:
	\begin{description}
		\item[Fully specified] $\to$ we use \emph{offline} algorithms to \emph{plan} $\pi$
			% This is typically achieved using a Markov Decision Procress.
		\item[Unknown] $\to$ we use \emph{online} algorithms to \emph{learn} $\pi$
	\end{description}
	\begin{remark} Learning is sometimes referred to as \emph{online} planning.\end{remark}
	\vspace{-5pt}
	\tags{Uncertainty, Policy Search}
\end{note}

\begin{note}{Markov Decision Process}
	An MDP is a mathematical framework for non-deterministic decision \hyperref[note:Planning vs. Learning]{\it planning} problems.
	It consists of:
	\begin{itemize}
		\item a set of states $S$, a start state $s_0$ and terminal states $\subseteq S$
		\item a set of actions $A(s)$ for each state $s \in S$
		\item a \emph{Markovian} (memoryless) transition model $P(s' | s, a)$
		\item a reward function $R(s,a,s')$ and a discount factor $\gamma$
	\end{itemize}
	\tags{Uncertainty, Policy Search, Offline Planning}
\end{note}

\begin{note}{Finite Horizons and Discounting}
	To prevent policies $\pi$ yielding infinite utility we introduced finite horizons or discounting.
	The ladder results in \emph{discounted rewards} $\gamma^t R(s, a, s')$ for actions taken at time $t$.
	We then obtain a discounted utility function:
	$$
		U(\mathbf{h}) = R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + ... < \frac{R_{max}}{1 - \gamma}
	$$
	\begin{remark}
		Finite horizon decision problems yield \emph{non-stationary} policies, i.e.
		policies depending on time $\pi : S \times T \to A$.
	\end{remark}
	\vspace{-5pt}
	\tags{Uncertainty, Policy Search, Offline Planning}
\end{note}

\begin{note}{Bellman Equation and Optimality}
	An optimal policy $\pi^*$ is one that satisfies the \emph{Bellman Equation}:
	\begin{itemize}
		\item $V^*(s) = \max_a  \sum_{s' \in S} P(s' | s, a) \left\{ R(s,a,s') + \gamma V^*(s') \right\}$
		\item $Q^*(s, a) = \sum_{s' \in S} P(s' | s, a) \left\{ R(s,a,s') + \gamma V^*(s') \right\}$
		\item $\pi^*(s) = \text{arg}\max_a Q^*(s, a)$
	\end{itemize}
	\begin{remark} The final line is called \emph{Policy Extraction}. \end{remark}
	\vspace{-5pt}
	\tags{Uncertainty, Policy Search, Offline Planning, MDP}
\end{note}

\begin{note}{Value Iteration}
	Value Iteration is an DP algorithm for computing the values $V^*(s)$ using the Bellman update
	until convergence:
	$$
		V^{k+1}(s) = \max_a  \sum_{s' \in S} P(s' | s, a) \left\{ R(s,a,s') + \gamma V^k(s') \right\}
	$$
	\begin{remark} Equivalent to a depth-$k$ expectimax search on the Markov Decision Process search tree.
	\end{remark}
	\vspace{-5pt}
	\tags{Uncertainty, Policy Search, Offline Planning, MDP}
\end{note}

% Convergence of VI ?

\begin{note}{Policy Iteration}
	Policy Iteration is an algorithm for computing the optimal policy $\pi^*$ directly using an initial policy $\pi_0$ and alternating:
	\begin{enumerate}
		\item {\bf $\pi$ evaluation}: compute $V^{\pi_i}(s)$ for each state.
		\item {\bf $\pi$ improvement}: $\pi_{i+1}(s) = \text{arg}\max_a Q^{\pi_i}(s, a)$
	\end{enumerate}
	\begin{remark} $\pi$ evaluation can be done using a simplified value iteration algorithm
		or solving $|S|$ linear equations using algebra.
	\end{remark}
	\vspace{-5pt}
	\tags{Uncertainty, Policy Search, Offline Planning, MDP}
\end{note}

\begin{note}{Simplified Value Iteration}
	This algorithm uses a simplified Bellman update to evaluate a given policy $\pi$ for Policy Iteration.
	$$
		V^{\pi}_{k+1}(s) =  \sum_{s' \in S} P(s' | s, \pi(s)) \left\{ R(s,\pi(s),s') + \gamma V^{\pi}_k(s') \right\}
	$$
	\vspace{-10pt}
	\tags{Uncertainty, Policy Search, Offline Planning, MDP, Policy Iteration}
\end{note}

% Async Policy Improvement

% Partially Observable MDP

% Bayesian Decision Network

% Game Theory
\begin{note}{Game Theory}
	Uncertainty in an environment can be caused by non-deterministic actions or by
	the presence of multiple agents in the environment.
	In the contest of games we talk about \emph{strategies} rather than policies.
	\vspace{5pt}

	See the \textbf{Game Theory} \noteref notes for more details on decision problems in multi-agent environments.
	\tags{Uncertainty, Strategy Search}
\end{note}

\begin{note}{Reinforcement Learning}
	RL is an \emph{online} planning process for unknown stochastic decision problems which \emph{explores} to collect \emph{reinforcements}.
	\begin{description}
		\item[Passive] $\to$ learn the state values $V^{\pi_0}(s)$ of a fixed policy $\pi_0$
		\item[Active] $\to$ learn and \emph{use} an estimate of the optimal policy $\pi^*$
	\end{description}
	\vspace{-5pt}
	\tags{Uncertainty, Policy Search, Reinforcement Learning}
\end{note}

\begin{note}{Machine and Statistical Learning}
	See the \textbf{Computational Statistics} \noteref and \textbf{Data Mining} \noteref notes
	for more details on \emph{supervised} and \emph{unsupervised} learning.
	
	\tags{Uncertainty, Policy Search, Reinforcement Learning}
\end{note}

%% Relationship with supervised learning!

\begin{note}{Exploration vs Exploitation}
	Active RL algorithms use an \emph{exploration} policy $\pi_e$ which captures a trade-off between:
	\begin{description}
		\item[Exploration] $\to$ improve the estimate of the true model
		\item[Exploitation] $\to$ maximize rewards according to $\hat{\pi}^*$
	\end{description}
	\begin{remark}
		Passive learning for $\pi_0$ is equivalent to active learning with constant exploration policy $\pi_e = \pi_0$.
	\end{remark}
	\vspace{-5pt}
	\tags{Uncertainty, Policy Search, Reinforcement Learning}
\end{note}

\begin{note}{Direct Utility Learning}
	\begin{mdframed}[linecolor=black!25!white]
		\emph{Passive \& model-free} RL
	\end{mdframed}
	\begin{itemize}
		\item Follow a fixed policy $\pi_0$ to collect pairs $(s, V^{\pi_0}_k(s))$ where $V^{\pi_0}_k(s)$ is the sum of rewards
		received in episode $k$ from state $s$.
		\item Estimate $V^{\pi_0}(s)$ by averaging all $V^{\pi_0}_k(s)$ containing $s$.
	\end{itemize}
	\begin{remark} Similar to supervised learning for $\{ s \to  V^{\pi_0}_k(s) \}$ data.\end{remark}
	\vspace{-5pt}
	\tags{Uncertainty, Policy Search, Reinforcement Learning}
\end{note}

\begin{note}{Adaptive Dynamic Programming}
	\begin{mdframed}[linecolor=black!25!white]
		\emph{Active \& model-based} RL
	\end{mdframed}
	\begin{itemize}
	\item collect samples $(s, \pi_e(s), s', R)$ through $\pi_e$
	\item estimate the reward $\hat{R}$ and transition function $\hat{T}$
	\item use \emph{offline} planning to compute $\hat{V}^{\pi_0}(s)$ or $\hat{\pi}^*$
	\end{itemize}
	\begin{remark} Similar to supervised learning for $\{ (s, a) \to (s',R) \}$ data.
	\end{remark}
	\vspace{-5pt}
	\tags{Uncertainty, Policy Search, Reinforcement Learning}
\end{note}

% Bayesian RL
%\begin{note}{Bayesian RL}
%	\begin{mdframed}[linecolor=black!25!white]
%		\emph{Active \& model-based}
%	\end{mdframed}
%	\begin{itemize}
%		\item Define prior probabilities $P(h)$ that model $h$ is correct
%		\item Given evidence $\mathbf{e}$ from $\pi_e$ compute posterior $P(h|\mathbf{e})$
%		\item Once learning is complete, compute the expected utility $V_h^{\pi}$ of $\pi$ in model $h$ averaged over all starting states.
%	\end{itemize}
%		$$
%			\pi^* = \text{arg}\max_\pi \sum_h P(h | \mathbf{e}) V_h^{\pi}
%		$$
%	\vspace{-10pt}
%	\tags{Uncertainty, Policy Search, Reinforcement Learning}
%\end{note}
%
%\begin{note}{Robust Control Theory}
%	\begin{mdframed}[linecolor=black!25!white]
%		\emph{Active \& model-based}
%	\end{mdframed}
%		\begin{itemize}
%		\item Define $\mathcal{H}$ as the set of possible models
%		\item Once learning is complete, compute the expected utility $V_h^{\pi}$ of $\pi$ in model $h$ averaged over all starting states.
%	\end{itemize}
%		$$
%			\pi^* = \text{arg}\max_\pi \min_h  V_h^{\pi}
%		$$
%	\vspace{-10pt}
%	\tags{Uncertainty, Policy Search, Reinforcement Learning, Active}
%\end{note}

\begin{note}{Temporal Difference Learning}
	\begin{mdframed}[linecolor=black!25!white]
		\emph{Passive \& model-free} RL
	\end{mdframed}
	Under fixed $\pi$, update $\hat{V}^\pi(s)$ for every sample $R(s, \pi(s), s')$ using
	an exponential running average with \emph{learning rate} $\alpha$:
	\begin{align*}
		&\Omega \triangleq R(s, \pi(s), s') + \gamma \hat{V}^\pi(s')\\
		&\hat{V}^\pi(s) \leftarrow (1 - \alpha) \hat{V}^\pi(s) + \alpha \Omega = \hat{V}^\pi(s) + \alpha( \Omega - \hat{V}^\pi(s) )
	\end{align*}
	\vspace{-10pt}
	\tags{Uncertainty, Policy Search, Reinforcement Learning}
\end{note}

% Priority Sweeping

\begin{note}{Q-Learning}
	\begin{mdframed}[linecolor=black!25!white]
		\emph{Active \& model-free} RL
	\end{mdframed}
	Learn the q-state values $\hat{Q}(s, \pi_e(s))$ for every sample $R(s, \pi_e(s), s')$ using
	a running average with \emph{learning rate} $\alpha$:
	\begin{align*}
		&\textstyle \Omega \triangleq R(s, a, s') + \gamma \max_{a'} \hat{Q}(s',a')\\
		&\textstyle \hat{Q}(s,a) \leftarrow (1 - \alpha) \hat{Q}(s,a) + \alpha \Omega = \hat{Q}(s,a) + \alpha ( \Omega - \hat{Q}(s,a))
	\end{align*}
	\begin{remark} $\max$ over q-state values $\implies$ this is \emph{off-policy} learning.
	\end{remark}
	\vspace{-5pt}
	\tags{Uncertainty, Policy Search, Reinforcement Learning}
\end{note}

\begin{note}{Epsilon Greedy}
	$\epsilon$-Greedy is an exploration policy $\pi_e$ for active RL which explores
	with (decaying) probability $\epsilon$ and exploits otherwise.
	$$
		\pi_e(s) = \begin{cases*}
			\text{random}\left\{ A(s) \right\} & \text{ with probability } $\epsilon$\\
			\hat{\pi}^*(s) & \text{ otherwise}
		\end{cases*}
	$$
	\begin{remark} A greedy agent uses 0-greedy exploration.
		Using $\epsilon > 0$ guarantees convergence to true utilities and optimal policy.
	\end{remark}\vspace{-5pt}
	\tags{Uncertainty, Policy Search, Reinforcement Learning, Exploration}
\end{note}

\begin{note}{Exploration Function}
	An exploration function $f(s, a)$ encourages exploration by
	assigning optimistic values $Q^+(s,a)$ to q-states with low attempts $N(s,a)$.
	\begin{align*}
		& \textstyle Q^+(s,a) = \sum_{s' \in S} P(s' | s, a) \{ R(s') + \max_{a'} f(s', a') \} \\
		& \textstyle f(s,a) = Q^+(s,a) + k \cdot N(s,a)^{-1} \quad\quad \text{\footnotesize(example)}
	\end{align*}
	\begin{remark} Then the exploration policy is just $\pi_e = \hat{\pi}^*$.
		This yields faster convergence to the optimal policy, but not values.
	\end{remark}\vspace{-5pt}
	\tags{Uncertainty, Policy Search, Reinforcement Learning, Exploration}
\end{note}

\begin{note}{Exploration Function for Q-Learning}
	An exploration function $f(s, a)$ can be integrated into the q-learning update
	as follows:
	\begin{align*}
		&\textstyle \Omega \triangleq R(s, a, s') + \gamma \max_{a'} f(s',a')\\
		&\textstyle Q^+(s,a) \leftarrow (1 - \alpha) Q^+(s,a) + \alpha \Omega 
	\end{align*}
	\vspace{-10pt}
	\tags{Uncertainty, Policy Search, Reinforcement Learning, Exploration}
\end{note}


% GLIE
% Multi-Armed Bandits 

\begin{note}{Approximate RL}
	Function approximation for \emph{model-free} RL uses feature representations $\mathbf{f}(s)$
	or $\mathbf{f}(s,a)$ to \emph{learn} a weight vector $\mathbf{w}$.
	$$ \hat{V}(s) = \mathbf{w}^\top \mathbf{f}(s) \quad\quad \hat{Q}(s,a) =  \mathbf{w}^\top \mathbf{f}(s,a) $$
	\begin{remark} Approximate reinforcement learning in a fully observable environment is similar to \emph{supervised learning}.
	\end{remark}
	\vspace{-5pt}
	\tags{Uncertainty, Policy Search, Reinforcement Learning, Approximation}
\end{note}

\begin{note}{Widrow-Huff Rule}
	The Widrow-Huff rule for \emph{online least squares} can be used learn weights $\mathbf{w}$ of $\hat{G}(x) = \mathbf{w}^\top \mathbf{f}(x)$
	with every sample $g(x)$ for some $x$
	\begin{align*}
		& \textstyle E(x) =(\hat{G}(x) - g(x))^2 / 2\\
		& \textstyle w_i \leftarrow w_i - \alpha \cdot \partial E(s)/\partial w_i = w_i + \alpha (g(x) - \hat{G}(x)) f_i(x)
	\end{align*}
	\vspace{-10pt}
	\tags{Uncertainty, Policy Search, Reinforcement Learning, Approximation}
\end{note}

\begin{note}{Approximate TD Learning}
	Using feature representation $\mathbf{f}(s)$ for states, update weight vector $\mathbf{w}$
	for every sample $R(s, a, s')$:
	\begin{align*}
		&\textstyle \Delta \triangleq R(s, a, s') + \gamma \hat{V}(s') - \hat{V}(s)\\
		&\textstyle w_i \leftarrow w_i + \alpha \Delta f_i(s)
	\end{align*}%
	\begin{remark} Widrow-Huff with $g(s) = R(s, a, s') + \gamma \hat{V}(s')$.\end{remark}
	\vspace{-5pt}
	\tags{Uncertainty, Policy Search, Reinforcement Learning, Approximation}
\end{note}


\begin{note}{Approximate Q-Learning}
	Using feature representation $\mathbf{f}(s,a)$ for q-states, update weight vector $\mathbf{w}$
	for every sample $R(s, \pi_e(s), s')$:
	\begin{align*}
		&\textstyle \Delta \triangleq R(s, a, s') + \gamma \max_{a'} \hat{Q}(s',a') - \hat{Q}(s, a)\\
		&\textstyle w_i \leftarrow w_i + \alpha \Delta f_i(s, a)
	\end{align*}%
	\begin{remark} Widrow-Huff with $g(s,a) = R(s, a, s') + \max_{a'} \hat{Q}(s',a') $.\end{remark}
	\vspace{-5pt}
	\tags{Uncertainty, Policy Search, Reinforcement Learning, Approximation}
\end{note}

% Policy Search

%%%%%%%%%%%%%%%%%%% TODO

\begin{question}{Todo}
	\begin{itemize}
		\item Clear Categories (unsupervised, reinforcement...)
		\item Generalization
		\item Cross-Validation
		\item Momentum SGD
		\item AdaGrad
		\item RMSProp
		\item Genetic Algorithm
		\item Neural Networks
		\item Back-propagation
		\item Gradient Boosting Machines
		\item CNNets
		\item k-Means
		\item Lloyd's Algorithm
		\item Logistic Regression - produces probabilities (what are logits?)
		\item sigmoid
		\item k-Nearest Neighbors
		\item PCA
		\item Dimension Reduction (SVD,...)
		\item Clustering
		\item Unsupervised Learning
		\item GMM
		\item Locality Sensitive Hashing
		\item GAN (Generative adversarial network)
		\item Keras example of NN
		\item Log-loss or cross entropy
		\item Feature Engineering
		\item Feature Crosses
		\item Embeddings
		\item Precision + Recall 
		\item Discriminant Analysis
		\item Maximum Likelihood Estiamtion (MLE)
		\item maximum a posteriori estimation (MAP)
		\item Expectation Maximization (EM)
		\item Naive Bayes Classifier
	\end{itemize}
\end{question}

% 14 Probabilistic Reasoning
% 15 Probabilistic Reasoning over Time
% 16 Making Simple Decisions (maybe)
% 20 Learning Probabilistic Models

% III: Knowledge, Reasoning and Planning

\begin{note}{MNIST in Keras}
	Loading and inspecting the {\it MNIST} dataset in Python using packages \incode{keras} and \incode{Tensorflow}
	are done as follows.
	\begin{largecode}
 from keras.datasets import mnist
 (train_images, train_labels),
 	(test_images, test_labels) = mnist.load_data()
	\end{largecode}
	\vspace{-5pt}
	\begin{largecode}
 plt.imshow(train_images[0], cmap=plt.cm.binary) 
 plt.show()
	\end{largecode}
	\begin{remark} For Mac OS \incode{10.10} use \incode{tf == 1.4.0} and \incode{keras == 2.1.3}. \end{remark}\vspace{-5pt}
	\tags{Deep Learning, MNIST, Keras, Tensorflow, Python}
\end{note}

%% HOW to build a simple model
%% how to compile it, then train/fit it! ~ similar to R

\begin{note}{Softmax Function}
	The softmax or {\it normalized exponential} function $\sigma$ is a generalization of the {\it logistic function}
	which "smashes" an arbitrary vector to a categorical distribution, $\sigma : \bm z \in \mathbb{R}^K \to (0, 1)^K$.
	$$
		\sigma(\bm z)_i = \frac{e^{z_i}}{\sum_j e^{z_j}}, \quad \quad \sum_i \sigma(\bm z)_i = 1
	$$
	\begin{remark} $\sigma$ is used in probabilistic \hyperref[note:Supervised Learning]{\it multiclass classification}. %,  linear discriminant analysis, naive Bayes classifiers. %, and artificial neural networks.
	\end{remark}
	\vspace{-5pt}
	\tags{Deep Learning, Neural Network, Activation}
\end{note}

\begin{note}{Sigmoid Function}
	The sigmoid function is the inverse of the \emph{logit} function
	and maps arbitrary real values into the unit interval, $f:  \mathbb{R} \to [0, 1]$.
	$$
		f(x) = \frac{1}{1+ e^{-x}} = \frac{e^x}{1 + e^x}
	$$
	\begin{remark} Used in \hyperref[note:Logistic Regression]{\it logistic regression} for probabilistic classification.\end{remark}
	\vspace{-5pt}
	\tags{Logistic Regression}
\end{note}

\begin{note}{Gradient Descent}
	The \emph{optimization} problem of minimizing a function $f(\bm w)$ can be solved
	using gradient descent, i.e. applying the negative gradient
	proportionally to some \emph{learning rate} $\eta$.
	$$
		\bm w^{(t+1)} \leftarrow \bm w^{(t)} - \eta \nabla f(\bm w^{(t)})
	$$
	\begin{remark} Due to computational inefficiency of determining $\nabla f(\bm w)$,
		\hyperref[note:Stochastic Gradient Descent]{\it stochastic or mini-batch} gradient descent
		is preferred in practice. \end{remark}
	\begin{remark} In machine learning, loss functions $l(\bm\theta \vert \bm X, \bm y)$ are defined for model parameters $\bm \theta$
		given training data $(\bm X, \bm y)$.\end{remark}
	\vspace{-5pt}
	\tags{Optimization, Gradient Descent}
\end{note}

\begin{note}{Stochastic Gradient Descent}
	Given an additive loss $f(\bm w) = \alpha \sum f_i (\bm w)$,
	stochastic \hyperref[note:Gradient Descent]{\it gradient descent} iteratively applies the gradient for a single function $f_i$:
	$$
		\bm w^{(t+1)} \leftarrow \bm w^{(t)} - \eta \nabla f_i(\bm w^{(t)})
	$$
	Using the gradient $\nabla f_S$ for the loss incurred by a larger subset $f_S = \sum_{S} f_i$ is referred to as \emph{mini-batch} SGD.
	\begin{remark}
		In practice the loss function is additive in \emph{sample points},
		thus $f_i$ corresponds to the isolated loss incurred from $\bm x_i$.
		Therefore gradient descent becomes an \emph{online} optimization algorithm.
	\end{remark}
	\vspace{-5pt}
	\tags{Search, Optimization, Gradient Descent, SGD}
\end{note}

\begin{note}{Supervised Learning}
	A supervised learning problem is defined by training data $(\bm X, \bm y)$ of
	feature vectors $\bm x_i$ and response variables $y_i = f(\bm x_i) + \epsilon$.
	\begin{mdframed}[linecolor=black!25!white]
		The goal is to find a \emph{good} function estimate $\hat f$ for $f$.
	\end{mdframed}
	\begin{description}
		\item[Numerical] values $y_i$ $\longrightarrow$ \emph{regression} analysis.
		\item[Categorical] labels $y_i$ $\longrightarrow$ \emph{classification} problem.
	\end{description}
	
	Parametric methods assume an underlying model
	$f(\bm x) = m(\bm x \vert \bm \theta)$ and learn the parameters $\hat{\bm \theta}$
	by minimizing a \emph{loss} function $l(\bm \theta \vert \bm X, \bm y)$.
	
	\tags{Supervised Learning, Regression, Classification, Estimation}
\end{note}

\begin{draft}{Supervised Learning Todo}
	\begin{itemize}
		\item Generalization Error
		\item Over-fitting
	\end{itemize}
\end{draft}

\begin{note}{Regularization}
	Regularization is the process of penalizing the complexity of a model $\hat f$ to prevent \emph{overfitting} to training data $(\bm X, \bm y)$.
	$$ g(\bm \theta \:\vert\: \bm X, \bm y) = l(\bm \theta \:\vert\: \bm X, \bm y ) + \lambda R(\bm \theta)$$
	where $g$ is a regularized objective function applying a weighted regularizer term $R(\cdot)$ to a given loss function $l$.
	\begin{example}
		\hyperref[note:Tikhonov Regularization]{\it Tikhonov}, 
		\hyperref[note:Lasso Regularization]{\it Lasso} and
		\hyperref[note:Elastic Net Regularization]{\it Elastic Net}.
	\end{example}
	\vspace{-5pt}
	\tags{Optimization, Generalization, Overfitting}
\end{note}

\begin{note}{Bias-Variance Tradeoff}
	The bias-variance decomposition in \hyperref[note:Supervised Learning]{\it supervised learning}
	is expressed through the expected generalization error:
	$$
		\underbrace{\mathds{E}[(f(x) - \hat{f}(x))^2]}_{\text{MSE}(x)} = (\underbrace{\mathds{E}[\hat{f}(x)] - f(x)}_{\text{Bias}})^2
		+ \underbrace{\mathds{E}[\hat{f}(x)^2] - E[\hat{f}(x)]^2}_{\text{Var}(\hat{f}(x))}
	$$
	\begin{remark} Optimizing this trade-off is called \hyperref[note:Regularization]{\it regularization}.
	\end{remark}
	\vspace{-5pt}
	\tags{Supervised Learning}
\end{note}

\begin{note}{Tikhonov Regularization}
	Also known as \emph{ridge regression} and \emph{weight decay},
	the Tikhonov method applies a $L_2$ norm-penality \hyperref[note:Regularization]{\it regularization} term.
	$$
		 g(\bm \theta \:\vert\: \bm X, \bm y) = l(\bm \theta \:\vert\: \bm X, \bm y ) + \lambda \| \bm \Gamma \bm \theta \|_2^2
	$$
	where $\bm \Gamma$ is the Tikhonov matrix. Using the identiy matrix $\bm \Gamma = \alpha \bm I$ is
	known as $L_2$ regularization.
	
	\tags{Optimization, Regularization}
\end{note}

\begin{note}{Lasso Regularization}
	Also known as $L_1$ regularization, the Lasso method performs variable selection and \hyperref[note:Regularization]{\it regularization}
	using $L_1$ norm-penalty.
	$$
		 g(\bm \theta \:\vert\: \bm X, \bm y) = l(\bm \theta \:\vert\: \bm X, \bm y ) + \lambda \| \bm \theta \|_1
	$$
	\vspace{-10pt}
	\tags{Optimization, Regularization}
\end{note}

\begin{note}{Elastic Net Regularization}
	Elastic Net is a \hyperref[note:Regularization]{\it regularization} method that linearly
	combines the $L_1$ and $L_2$ norm penalties of \hyperref[note:Tikhonov Regularization]{\it Tikhonov} and 
	\hyperref[note:Lasso Regularization]{\it Lasso}.
	$$
		 g(\bm \theta \:\vert\: \bm X, \bm y) = l(\bm \theta \:\vert\: \bm X, \bm y ) + \lambda_1 \| \bm \theta \|_1 + \lambda_2 \| \bm \theta \|_2^2
	$$
	\vspace{-10pt}
	\tags{Optimization, Regularization}
\end{note}

\begin{note}{Perceptron}
	A perceptron $\hat f$ is a non-probabilistic linear \hyperref[note:Supervised Learning]{\it binary classifier} for feature vectors $\bm x \in \mathbb{R}^n$
	using a \emph{decision boundary} defined by a weight vector $\bm w \in \mathbb{R}^n$ and a bias $b \in \mathbb{R}$. 
	$$ \hat f(\bm x) = \text{sign}(\bm w^\intercal \bm x + b) $$
%	Learning perceptron $\hat f$ using (\hyperref[note:Stochastic Gradient Descent]{\it stochastic}) \hyperref[note:Gradient Descent]{\it gradient descent}
%	on some loss function $l(\bm w, b \:\vert\: \bm X, \bm y)$ requires no learning rate $\eta$.
	\begin{remark}
		A perceptron is equivalent to an \hyperref[note:Artificial Neural Network]{\it artificial network} neuron with a \emph{heaviside} activation function.
		%\emph{Multi-layer} perceptrons are equivalent to feedforward \emph{neural networks} with a \emph{heaviside} activation function.
	\end{remark}
	\vspace{-5pt}
	\tags{Supervised Learning, Classification}
\end{note}

\begin{note}{Support Vector Machine}
	An SVM $\hat f$ %, also called \hyperref[note:Perceptron]{\it perceptron} of \emph{optimal stability}
	is a non-probabilistic linear \hyperref[note:Supervised Learning]{\it binary classifier}
	for feature vectors $\bm x \in \mathbb{R}^n$ using a \emph{decision hyperplane} $(\bm w, b) \in \mathbb{R}^n \times \mathbb{R}$
	with maximized \emph{decision margin} $2/\| \bm w\|$.
	$$ \hat f(\bm x) = \text{sign}(\bm w^\intercal \bm x + b) $$
	Support Vector Machines are learned with either \hyperref[note:Hard Margin SVM]{\it hard} or \hyperref[note:Soft Margin SVM]{\it soft} margins
	depending on the linear-separability of the data $\bm X$.
	
	\begin{remark}
		Also called \hyperref[note:Perceptron]{\it perceptron} of \emph{optimal stability}.
	\end{remark}
	\vspace{-5pt}
	\tags{Supervised Learning, Classification, Linear Classifier, Binary Classifier, SVM}
\end{note}

\begin{note}{Hard Margin SVM}
	For linearly separable classification problems, a \hyperref[note:Support Vector Machine]{\it Support Vector Machine} can be trained
	to maximize the hyperplane margin $2/\| \bm w\|$ on a normalized dataset as follows:
	$$
		\min_{\bm w} \, \| \bm w \| \quad \text{ s.t } \quad y_i \left(\bm w^\intercal \bm x_i + b\right) \geq 1 \quad \forall \bm x_i \in \bm X
	$$
	\vspace{-10pt}
	\tags{Supervised Learning, Classification, Linear Classifier, Binary Classifier, SVM, Hard Margin}
\end{note}

\begin{note}{Soft Margin SVM}
	For linearly inseparable classification problems, a \hyperref[note:Support Vector Machine]{\it Support Vector Machine} can be trained
	with a soft margin by introducing \emph{hinge} loss instead of hard separability constraints.
	$$
		\min_{\bm w} \, \lambda \| \bm w \|  +  \frac{1}{n} \sum_{i} \max \left\{0, 1 - y_i \left(\bm w^\intercal \bm x_i + b\right)\right\} 
	$$
%	This formulation is called the regularized hinge loss minimization.
	This can be solved using sub-\hyperref[note:Gradient Descent]{\it gradient descent}, or 
	a norm constrained formulation can be used for projected gradient descent.
	
	\tags{Supervised Learning, Classification, Linear Classifier, Binary Classifier, SVM, Hard Margin}
\end{note}

\begin{draft}{SVM Todo}
	\begin{itemize}
		\item Kernel Trick for Hard Margin SVM
		\item Optimization of SVM (using norm-constrained)
		\item Online SVM?
		\item How do we learn Hard Margin SVM?
	\end{itemize}
\end{draft}

\begin{note}{Linear Regression}
	Linear regression is a  \hyperref[note:Supervised Learning]{\it regression analysis} method
	assuming a linear relationship between the response variables $y_i$ and the feature vectors $\bm x_i$.
	$$ f(\bm x) = \bm\beta^\intercal \bm x \quad \text{i.e.} \quad \bm y = \bm X \bm\beta + \bm \epsilon $$
%	The model parameters are usually fitted by minimizing the sum of squared residuals, corresponding to by the {\it least squares estimator}.
%	$$ \hat {\bm \beta} = \text{arg}\max_{\bm\beta} \| \bm y - \bm X \bm\beta \|^2 $$
	The model parameters are usually fitted by minimizing the sum of squared residuals, corresponding to by the {\it least squares estimator}.
	$$ \hat {\bm \beta} = \text{arg}\max_{\bm\beta} \| \bm y - \bm X \bm\beta \|^2 $$
	\vspace{-10pt}
	%\begin{remark} Equivalent to a \hyperref[note:Artificial Neural Network]{\it artificial network} neuron without activation. \end{remark}
	\tags{Supervised Learning, Regression, Linear Regression, Least Squares Estimator}
\end{note}

\begin{note}{Logistic Regression}
	Logistic regression produces a probabilistic \hyperref[note:Supervised Learning]{\it binary classifier}
	by applying the \hyperref[note:Sigmoid Function]{\it sigmoid} function to a linear model $\bm \beta$.
	$$
		\hat \pi(\bm x) = \left[1 + e^{- \bm \beta^\intercal \bm x} \right]^{-1}
	$$
	The decision threshold $\tau$ is chosen based on \hyperref[note:Precision]{\it precision} and \hyperref[note:Recall]{\it recall}.
	$\bm \beta$ can be estimated by \hyperref[note:Maximum Likelihood Estimator]{\it maximum likelihood}
	i.e. minimizing the negative log-likelihood, by using for example \hyperref[note:Gradient Descent]{\it gradient descent}.
	$$
		l(\bm \beta \vert \bm X, \bm y) = - \bm y^\intercal \log \hat {\bm \pi}(\bm X) - (\bm 1 - \bm y)^\intercal \log \left(\bm 1 - \hat {\bm \pi}(\bm X) \right)
	$$
	\begin{remark} This cost function $l$ is an instance of \hyperref[note:Cross Entropy]{\it cross entropy}. \end{remark}
	\vspace{-5pt}
	\tags{Supervised Learning, Classification, Linear Classifier, Binary Classifier, Probabilistic Classifier}
\end{note}

\begin{note}{Cross Entropy}
	The cross entropy $H$ is a measure quantifying the distance between two probability distributions $p$ and $q$. 
	For a discrete set $\bm X$:
	$$
		H(p, q) = - \sum_{\bm x} p(\bm x) \log q(\bm x)
	$$
	\begin{remark}
		Generalization of negative \hyperref[note:Logistic Regression]{\it log-likelihood}.
		Used as a loss function to train probabilistic \hyperref[note:Supervised Learning]{\it classifiers}.
	\end{remark}
	\tags{Optimization, Supervised Learning, Classification, Probabilistic Classification}
\end{note}

\begin{note}{Multinomial Logistic Regression}
	Multinomial logistic regression generalizes \hyperref[note:Logistic Regression]{\it logistic regression} for multiple classes
	using the \hyperref[note:Softmax Function]{\it softmax} function with $\bm B \in \mathbb{R}^{N \times K}$.
	$$
		\hat \pi(y = k \:\vert\: \bm x) = \sigma(\bm B \bm x)_k = \frac{e^{\bm \beta_k^\intercal \bm x}}{\sum_j e^{\bm \beta_j^\intercal \bm x}}
	$$
	$\bm B$ can again be fitted using maximum-likelihood estimation which equivalently minimizes the \hyperref[note:Cross Entropy]{\it cross entropy}.
	%Classification is then performed using $\text{arg}\max_k \hat \pi(y = k \:\vert\: \bm x)$.
	\begin{remark} Equivalent to a single layer \hyperref[note:Artificial Neural Network]{\it neural network} classifier. \end{remark}
	\vspace{-5pt}
	\tags{Supervised Learning, Classification, Linear Classifier, Multiclass Classifier, Probabilistic Classifier}
\end{note}

\begin{note}{Linear Discriminant Analysis}
	LDA is used for probabilistic \hyperref[note:Supervised Learning]{\it multi-class classification} by
	assuming a $p$-dimensional Gaussian distribution 
	$ ( \bm x \:\vert\: y = j ) \sim \mathcal{N}_p(\bm \mu_j, \bm\Sigma) $.
	$$
		\hat\pi_k(\bm x) = \frac{f_k(\bm x) p_k}{\sum_j f_j(\bm x) p_j}
	$$
	where $p_j = P(y = j)$ and $f_j$ is the density of $\mathcal{N}_p(\bm \mu_j, \bm\Sigma)$.
	Standard \emph{moment estimators} can be used for $\hat{\bm \mu_j}, \hat{\bm\Sigma}$ and $\hat p_j$.
	The decision function $\hat c(\bm x) = \text{arg}\max_j \hat\delta_j(\bm x)$ is then linear in predictor $\bm x$:
	$$
		\hat\delta_k(\bm x) = (\bm x - \hat{\bm \mu}_j)^\intercal \hat{\bm\Sigma}^{-1} \hat{\bm\mu}_j + \log \hat p_j
	$$
	\begin{remark} Without the homoscedasticity $\bm\Sigma$ assumption we get \emph{quadratic} discriminant analysis. \end{remark}
	\vspace{-5pt}
	\tags{Supervised Learning, Classification}
\end{note}

%% Activation Functions:
% Logit function
% Relu Function
% Sigmoid Function

%% Loss functions
% Crossentropy

%% Common Models/Layers
% ...

%% Optimizer
% RMSprop

% soooo much stuff here

% Convolutional Layers
% Max pooling and stuff

\begin{draft}{Keras Neural Network}
	The following is a very high-level example of a \hyperref[note:Artificial Neural Network]{\it neural network}
	built and trained using \incode{keras}.
	\begin{largecode}
model = tf.keras.models.Sequential([ ... ])
model.compile(optimizer='adam',
	loss='sparse_categorical_crossentropy',
	metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5)
model.evaluate(x_test, y_test)
	\end{largecode}
	\vspace{-5pt}
	\tags{Keras, Deep Learning, Python, Example}
\end{draft}

\end{document}




